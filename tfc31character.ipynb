{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNbEC8qGwZv2FC1jshw83ua",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gwon0919/ML/blob/main/tfc31character.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1cENSnlBGty",
        "outputId": "2e8a8813-2480-4e8c-de7f-ff51fc72bb08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/pykwon/etc/master/rnn_test_toji.txt\n",
            "1588545/1588545 [==============================] - 0s 0us/step\n",
            "전체 글자 수 length of text : 695685\n",
            "제 1 편 어둠의 발소리\n",
            "1897년의 한가위.\n",
            "까치들이 울타리 안 감나무에 와서 아침 인사를 하기도 전에, 무색 옷에 댕기꼬리를 늘인 \n",
            "아이들은 송편을 입에 물고 마을길을 쏘\n",
            "['제 1 편 어둠의 발소리', '1897년의 한가위', '까치들이 울타리 안 감나무에 와서 아침 인사를 하기도 전에, 무색 옷에 댕기꼬리를 늘인 ', '아이들은 송편을 입에 물고 마을길을 쏘다니며 기뻐서 날뛴다 어른들은 해가 중천에서 좀 ', '기울어질 무렵 이래야, 차례를 치러야 했고 성묘를 해야 했고 이웃끼리 음식을 나누다 보면 ', '한나절은 넘는다 이때부터 타작마당에 사람들이 모이기 시작하고 들뜨기 시작하고 남정네 ', '노인들보다 아낙들의 채비는 아무래도 더디어지는데 그럴 수밖에 없는 것이 식구들 시중에 ', '음식 간수를 끝내어도 제 자신의 치장이 남아 있었으니까 이 바람에 고개가 무거운 벼이삭', '이 황금빛 물결을 이루는 들판에서는, 마음놓은 새떼들이 모여들어 풍성한 향연을 벌인다', ' 후우이이 요놈의 새떼들아! ', ' 극성스럽게 새를 쫓던 할망구는 와삭와삭 풀밭이 선 출입옷으로 갈아입고 타작마당에서 ', '굿을 보고 있을 것이다 추석은 마을의 남녀노유, 사람들에게뿐만 아니라 강아지나 돼지나 ', '소나 말이나 새들에게, 시궁창을 드나드는 쥐새? 끼까지 포식의 날인가 보다 ', ' 빠른 장단의 꽹과리 소리, 느린 장단의 둔중한 여음으로 울려퍼지는 징 소리는 타작마당', '과 거리가 먼 최참판댁 사랑에서는 흐느낌같이 슬프게 들려온다 농부들은 지금 꽃 달린 고', '깔을 흔들면서 신명을 내고 괴롭고 한스러운 일상을 잊으며 굿놀이에 열중하고 있을 것이', '다 최참판댁에서 섭섭찮게 전곡이 나갔고, 풍년에는 미치지 못했으나 실한 평작임엔 틀림이 ', '없을 것인즉 모처럼 허리끈을 풀어놓고 쌀밥에 식구들은 배를 두드렸을 테니 하루의 근심은 ', '잊을 만했을 것이다 ', ' 이날은 수수개비를 꺾어도 아이들은 매를 맞지 않는다 여러 달만에 솟증 풀었다고 느긋']\n",
            "['제', '1', '편', '어둠의', '발소리', '\\n', '1897년의', '한가위', '\\n', '까치들이']\n"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "path = keras.utils.get_file(\n",
        "    'rnn_test_toji.txt',\n",
        "    origin='https://raw.githubusercontent.com/pykwon/etc/master/rnn_test_toji.txt')\n",
        "text = open(path,'rb').read().decode(encoding='utf-8')\n",
        "print('전체 글자 수 length of text : {}'.format(len(text)))\n",
        "print(text[:100])\n",
        "\n",
        "import re\n",
        "\n",
        "def clean_str_func(string):   # 데이터 정제\n",
        "  string = re.sub(r\"[^가-힣0-9(),?! ]\",\"\", string)\n",
        "  string = re.sub(r\"!\",\"! \", string)\n",
        "  string = re.sub(r\"\\(\",\"\", string)\n",
        "  string = re.sub(r\"\\)\",\"\", string)\n",
        "  string = re.sub(r\"\\?\",\"? \", string)\n",
        "  string = re.sub(r\"\\s{2,}\",\" \", string)\n",
        "  string = re.sub(r\"\\'\",\"\", string)\n",
        "  return string\n",
        "\n",
        "# print(clean_str_func(\"abc?!, AB  12 나는' (간다) 34\"))   # ? ! , 12 나는 간다 34\n",
        "\n",
        "train_text = text.split('\\n')\n",
        "train_text = [clean_str_func(sentence) for sentence in train_text]\n",
        "print(train_text[:20])\n",
        "\n",
        "train_text_x =[]\n",
        "for sen in train_text:\n",
        "  train_text_x.extend(sen.split(' '))\n",
        "  train_text_x.append('\\n')\n",
        "\n",
        "train_text_x = [word for word in train_text_x if word != '']   # 단어1 단어2 단어3 ...\n",
        "print(train_text_x[:10])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 사전\n",
        "vocab = sorted(set(train_text_x))\n",
        "vocab.append('UNK')   # 단어 사전에 존재하지 않는 토큰은 <unk>로 처리한다.\n",
        "# 특수 토큰 < UNK > : 토크나이저가 모르는 단어를 만나면 unknown으로 처리하기 위한, 처리용 토큰\n",
        "# <eos>, <pad>, <sep>, <mask>\n",
        "print('{} unique words'.format(len(vocab)))   # 54048\n",
        "\n",
        "# 단어 인덱싱\n",
        "word2idx = { w:i for i, w in  enumerate(vocab)}\n",
        "# print(word2idx)  # {'\\n': 0, ',': 1, ',,?': 2, ',그년이,': 3, '00': 4, '00까지': 5, '1': 6, '10장': 7, ...\n",
        "idx2word = np.array(vocab)\n",
        "print(idx2word)    # ['\\n' ',' ',,?' ... '힘찼으며' '힝' 'UNK']\n",
        "text_as_int = np.array([word2idx[c] for c in train_text_x])\n",
        "print(text_as_int) # [43654     6 50166 ... 40598     0     0]\n",
        "\n",
        "print(train_text_x[:20])   # ['제', '1', '편', '어둠의', '발소리', '\\n', '1897년의', '한가위', '\\n', ...\n",
        "print(text_as_int[:20])    # [43654     6 50166 34431      21971     0    19           51388     0....\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnAKkU5FHhg8",
        "outputId": "0163be46-11f0-4141-9ea7-bc7c64495208"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "54048 unique words\n",
            "['\\n' ',' ',,?' ... '힘찼으며' '힝' 'UNK']\n",
            "[43654     6 50166 ... 40598     0     0]\n",
            "['제', '1', '편', '어둠의', '발소리', '\\n', '1897년의', '한가위', '\\n', '까치들이', '울타리', '안', '감나무에', '와서', '아침', '인사를', '하기도', '전에,', '무색', '옷에']\n",
            "[43654     6 50166 34431 21971     0    19 51388     0  6925 38396 32980\n",
            "  1062 37351 32849 40304 50929 43182 20097 37292]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset 작성\n",
        "seq_length = 25    # 25개의 단어가 주어질 경우 다음 단어를 예측\n",
        "example_per_epoch = len(text_as_int) // seq_length\n",
        "print(example_per_epoch)     # 7093\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "sentence_dataset= tf.data.Dataset.from_tensor_slices(text_as_int)  # 전체가 아니라 부분 데이터 읽기\n",
        "sentence_dataset = sentence_dataset.batch(seq_length + 1, drop_remainder=True)  # drop_remainder=True : 마지막 배치 크기를 무시\n",
        "# seq_length + 1 : 처음 25개 단어(feature)와 그 뒤에 나오는 정답(label)이될 한 단어를 합쳐 반환하기 위함\n",
        "\n",
        "for item in sentence_dataset.take(1):     # batch를 한 번씩 불러옴\n",
        "  print(item.numpy())                   # [43654  6 50166 34431    21971     0\n",
        "  print(idx2word[item.numpy()])         # ['제' '1' '편' '어둠의' '발소리' '\\n'\n",
        "\n",
        "print()\n",
        "def split_input_target(chunk):\n",
        "  return [chunk[:-1], chunk[-1]]       # [25단어], [1단어]\n",
        "\n",
        "train_dataset = sentence_dataset.map(split_input_target)\n",
        "for x, y in train_dataset.take(1):\n",
        "  print(idx2word[x.numpy()])\n",
        "  print(x.numpy())\n",
        "  print(idx2word[y.numpy()])\n",
        "  print(y.numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLMRTPQHPmb9",
        "outputId": "d979b4ac-7a98-45a9-96b2-71eeb0dbb1f7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7093\n",
            "[43654     6 50166 34431 21971     0    19 51388     0  6925 38396 32980\n",
            "  1062 37351 32849 40304 50929 43182 20097 37292 12523 11092     0 32768\n",
            " 29657 40995]\n",
            "['제' '1' '편' '어둠의' '발소리' '\\n' '1897년의' '한가위' '\\n' '까치들이' '울타리' '안' '감나무에'\n",
            " '와서' '아침' '인사를' '하기도' '전에,' '무색' '옷에' '댕기꼬리를' '늘인' '\\n' '아이들은' '송편을' '입에']\n",
            "\n",
            "['제' '1' '편' '어둠의' '발소리' '\\n' '1897년의' '한가위' '\\n' '까치들이' '울타리' '안' '감나무에'\n",
            " '와서' '아침' '인사를' '하기도' '전에,' '무색' '옷에' '댕기꼬리를' '늘인' '\\n' '아이들은' '송편을']\n",
            "[43654     6 50166 34431 21971     0    19 51388     0  6925 38396 32980\n",
            "  1062 37351 32849 40304 50929 43182 20097 37292 12523 11092     0 32768\n",
            " 29657]\n",
            "입에\n",
            "40995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = example_per_epoch // BATCH_SIZE\n",
        "BUFFER_SIZE = 5000\n",
        "\n",
        "# shuffle을 사용하면 epoch 마다 Dataset을 섞을 수 있다. 과적합 방지에 효과적\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "total_words = len(vocab)\n",
        "print(total_words)  # 53658\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(total_words, 100, input_length=seq_length), # 밀집벡터, 100차원으로\n",
        "    tf.keras.layers.LSTM(units=256, return_sequences=True),\n",
        "    tf.keras.layers.Dropout(rate=0.2),\n",
        "    tf.keras.layers.LSTM(units=256),\n",
        "    tf.keras.layers.Dropout(rate=0.2),\n",
        "    tf.keras.layers.Dense(units=total_words, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DYzXk44Un4K",
        "outputId": "0c70cba5-e992-46c0-b20a-624ab2964ded"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "54048\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 25, 100)           5404800   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 25, 256)           365568    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 25, 256)           0         \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 256)               525312    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 54048)             13890336  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20186016 (77.00 MB)\n",
            "Trainable params: 20186016 (77.00 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 단위 생성 모델 학습\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def testmodelFunc(epoch, logs):\n",
        "  if epoch % 5 != 0 and epoch !=49:   #  5의 배수 이거나 49이면 처리\n",
        "    return\n",
        "\n",
        "test_sentence = train_text[0]\n",
        "next_words = 100\n",
        "for _ in range(next_words):\n",
        "  test_text_x = test_sentence.split(' ')[-seq_length:]\n",
        "  test_text_x = np.array([word2idx[c] if c in word2idx else word2idx['UNK'] for c in test_text_x])\n",
        "  test_text_x = pad_sequences([test_text_x], maxlen=seq_length, padding='pre', value=word2idx['UNK'])\n",
        "  output_idx = np.argmax(model.predict(test_text_x)[0])  # 출력값 중에서 가장 값이 큰 인덱스 반환\n",
        "  test_sentence += ' ' + idx2word[output_idx]  # 출력 단어는 test_sentence에 누적해 다음 작업의 입력으로 활용\n",
        "\n",
        "  print()\n",
        "print(test_sentence)\n",
        "print()\n",
        "\n",
        "# epoch이 끝날 때 마다 testmodelFunc를 해 진행 결과를 출력.\n",
        "# fit 할 때(학습 도중) 학습 데이터가 predict 되는 과정을 확인해가며 작업하고 싶을 때사용\n",
        "testModelCb = tf.keras.callbacks.LambdaCallback(on_epoch_begin=testmodelFunc)\n",
        "\n",
        "# repeat() : input을 반복, 1개의 에폭의 끝과 다음 에폭의 시작에 상관없이 인자 만큼 반복함\n",
        "history = model.fit(train_dataset.repeat(), epochs=50, steps_per_epoch = steps_per_epoch,   # 한 에폭에 사용할 step 수를 지정. ex) 총 45개 sample이 있고 배치사이즈가 3이라면 15스텝으로 지정\n",
        "                   callbacks =[testModelCb], verbose=2)\n"
      ],
      "metadata": {
        "id": "VBDLSE6-rHme",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33d4578f-5ad0-410b-c63c-b578152228f7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 6s 6s/step\n",
            "\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 136ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 180ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 194ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 133ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 136ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 154ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 147ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "\n",
            "제 1 편 어둠의 발소리 영팔이가 많아지지 많아지지 많아지지 많아지지 풍겨왔다 함시로, 함시로, 풍겨왔다 천앙산같이 천앙산같이 천앙산같이 마당에 마당에 마당에 마당에 게고허허, 게고허허, 게고허허, 게고허허, 상사붙을란다! 거구, 놀음판이 와작와작 와작와작 꿈틀거리는 꿈틀거리는 꿈틀거리는 엽총에 노래도 노래도 노래도 노래도 만대까지 간혹 기로되 기로되 기로되 새끼들도 대추다 새끼들도 새끼들도 않으 무질서가 녁도 사왔다고? 사왔다고? 사왔다고? 사왔다고? 법식에 법식에 하얗고 하얗고 하얗고 이년이, 느끼거든 들었소 태이났이믄 내밀어놓고 글씨공부 부엌안을 거리 구걸하러 삼수아비를 덜게 덜게 조르는 꽂고 꽂고 양반님네 양반님네 양반님네 양반님네 시꺼멓게 시꺼멓게 시꺼멓게 수동이처 지쳤던 지쳤던 지쳤던 지쳤던 구석지에 실망을 실망을 사람가 사람가 불거질 불거질 세월일세 세월일세 세월일세 뒤었고 않아서인지 않아서인지 메주 울타리다 울타리다 새미에 모르는지 집앞에\n",
            "\n",
            "Epoch 1/50\n",
            "110/110 - 18s - loss: 10.1576 - accuracy: 0.1078 - 18s/epoch - 167ms/step\n",
            "Epoch 2/50\n",
            "110/110 - 5s - loss: 8.3749 - accuracy: 0.1071 - 5s/epoch - 46ms/step\n",
            "Epoch 3/50\n",
            "110/110 - 5s - loss: 7.9354 - accuracy: 0.1102 - 5s/epoch - 49ms/step\n",
            "Epoch 4/50\n",
            "110/110 - 3s - loss: 7.8725 - accuracy: 0.1057 - 3s/epoch - 27ms/step\n",
            "Epoch 5/50\n",
            "110/110 - 3s - loss: 7.6638 - accuracy: 0.1067 - 3s/epoch - 25ms/step\n",
            "Epoch 6/50\n",
            "110/110 - 2s - loss: 7.3807 - accuracy: 0.1092 - 2s/epoch - 21ms/step\n",
            "Epoch 7/50\n",
            "110/110 - 3s - loss: 7.1999 - accuracy: 0.1057 - 3s/epoch - 29ms/step\n",
            "Epoch 8/50\n",
            "110/110 - 3s - loss: 7.0138 - accuracy: 0.1102 - 3s/epoch - 28ms/step\n",
            "Epoch 9/50\n",
            "110/110 - 3s - loss: 6.8357 - accuracy: 0.1095 - 3s/epoch - 25ms/step\n",
            "Epoch 10/50\n",
            "110/110 - 2s - loss: 6.7568 - accuracy: 0.1040 - 2s/epoch - 22ms/step\n",
            "Epoch 11/50\n",
            "110/110 - 2s - loss: 6.7062 - accuracy: 0.1142 - 2s/epoch - 20ms/step\n",
            "Epoch 12/50\n",
            "110/110 - 4s - loss: 7.0546 - accuracy: 0.1089 - 4s/epoch - 33ms/step\n",
            "Epoch 13/50\n",
            "110/110 - 2s - loss: 6.6507 - accuracy: 0.1094 - 2s/epoch - 22ms/step\n",
            "Epoch 14/50\n",
            "110/110 - 2s - loss: 6.3430 - accuracy: 0.1132 - 2s/epoch - 20ms/step\n",
            "Epoch 15/50\n",
            "110/110 - 2s - loss: 6.0974 - accuracy: 0.1178 - 2s/epoch - 20ms/step\n",
            "Epoch 16/50\n",
            "110/110 - 3s - loss: 5.9320 - accuracy: 0.1169 - 3s/epoch - 23ms/step\n",
            "Epoch 17/50\n",
            "110/110 - 3s - loss: 5.7044 - accuracy: 0.1217 - 3s/epoch - 23ms/step\n",
            "Epoch 18/50\n",
            "110/110 - 3s - loss: 5.5990 - accuracy: 0.1212 - 3s/epoch - 24ms/step\n",
            "Epoch 19/50\n",
            "110/110 - 2s - loss: 5.3683 - accuracy: 0.1366 - 2s/epoch - 21ms/step\n",
            "Epoch 20/50\n",
            "110/110 - 2s - loss: 5.3424 - accuracy: 0.1293 - 2s/epoch - 22ms/step\n",
            "Epoch 21/50\n",
            "110/110 - 2s - loss: 5.1593 - accuracy: 0.1388 - 2s/epoch - 21ms/step\n",
            "Epoch 22/50\n",
            "110/110 - 3s - loss: 4.9640 - accuracy: 0.1534 - 3s/epoch - 24ms/step\n",
            "Epoch 23/50\n",
            "110/110 - 3s - loss: 4.8702 - accuracy: 0.1504 - 3s/epoch - 24ms/step\n",
            "Epoch 24/50\n",
            "110/110 - 3s - loss: 4.6757 - accuracy: 0.1625 - 3s/epoch - 26ms/step\n",
            "Epoch 25/50\n",
            "110/110 - 2s - loss: 4.5573 - accuracy: 0.1669 - 2s/epoch - 22ms/step\n",
            "Epoch 26/50\n",
            "110/110 - 2s - loss: 4.3943 - accuracy: 0.1814 - 2s/epoch - 20ms/step\n",
            "Epoch 27/50\n",
            "110/110 - 2s - loss: 4.2925 - accuracy: 0.1932 - 2s/epoch - 21ms/step\n",
            "Epoch 28/50\n",
            "110/110 - 2s - loss: 4.2436 - accuracy: 0.1977 - 2s/epoch - 21ms/step\n",
            "Epoch 29/50\n",
            "110/110 - 4s - loss: 4.1041 - accuracy: 0.2119 - 4s/epoch - 32ms/step\n",
            "Epoch 30/50\n",
            "110/110 - 2s - loss: 3.9302 - accuracy: 0.2332 - 2s/epoch - 21ms/step\n",
            "Epoch 31/50\n",
            "110/110 - 2s - loss: 3.8198 - accuracy: 0.2409 - 2s/epoch - 20ms/step\n",
            "Epoch 32/50\n",
            "110/110 - 2s - loss: 3.6707 - accuracy: 0.2547 - 2s/epoch - 20ms/step\n",
            "Epoch 33/50\n",
            "110/110 - 2s - loss: 3.5233 - accuracy: 0.2746 - 2s/epoch - 20ms/step\n",
            "Epoch 34/50\n",
            "110/110 - 2s - loss: 3.4196 - accuracy: 0.2940 - 2s/epoch - 22ms/step\n",
            "Epoch 35/50\n",
            "110/110 - 3s - loss: 3.2635 - accuracy: 0.3151 - 3s/epoch - 24ms/step\n",
            "Epoch 36/50\n",
            "110/110 - 2s - loss: 3.1484 - accuracy: 0.3386 - 2s/epoch - 20ms/step\n",
            "Epoch 37/50\n",
            "110/110 - 2s - loss: 3.0224 - accuracy: 0.3565 - 2s/epoch - 22ms/step\n",
            "Epoch 38/50\n",
            "110/110 - 2s - loss: 2.9348 - accuracy: 0.3821 - 2s/epoch - 20ms/step\n",
            "Epoch 39/50\n",
            "110/110 - 2s - loss: 2.8184 - accuracy: 0.4023 - 2s/epoch - 22ms/step\n",
            "Epoch 40/50\n",
            "110/110 - 3s - loss: 2.7053 - accuracy: 0.4276 - 3s/epoch - 23ms/step\n",
            "Epoch 41/50\n",
            "110/110 - 3s - loss: 2.5812 - accuracy: 0.4615 - 3s/epoch - 28ms/step\n",
            "Epoch 42/50\n",
            "110/110 - 2s - loss: 2.4970 - accuracy: 0.4874 - 2s/epoch - 20ms/step\n",
            "Epoch 43/50\n",
            "110/110 - 2s - loss: 2.3766 - accuracy: 0.5121 - 2s/epoch - 20ms/step\n",
            "Epoch 44/50\n",
            "110/110 - 2s - loss: 2.2869 - accuracy: 0.5237 - 2s/epoch - 20ms/step\n",
            "Epoch 45/50\n",
            "110/110 - 2s - loss: 2.1745 - accuracy: 0.5635 - 2s/epoch - 21ms/step\n",
            "Epoch 46/50\n",
            "110/110 - 3s - loss: 2.1007 - accuracy: 0.5866 - 3s/epoch - 23ms/step\n",
            "Epoch 47/50\n",
            "110/110 - 3s - loss: 1.9983 - accuracy: 0.6087 - 3s/epoch - 23ms/step\n",
            "Epoch 48/50\n",
            "110/110 - 3s - loss: 2.0463 - accuracy: 0.6078 - 3s/epoch - 25ms/step\n",
            "Epoch 49/50\n",
            "110/110 - 2s - loss: 1.9615 - accuracy: 0.6335 - 2s/epoch - 21ms/step\n",
            "Epoch 50/50\n",
            "110/110 - 2s - loss: 1.8535 - accuracy: 0.6585 - 2s/epoch - 21ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(history.history['loss'][-1])\n",
        "print(history.history['accuracy'][-1])\n",
        "\n",
        "model.save('tfc31model.hfd5')"
      ],
      "metadata": {
        "id": "tYpQw_amxj9w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69025051-bbcf-462f-b38f-862b465754af"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.8534656763076782\n",
            "0.6585227251052856\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('tfc31model.hfd5')\n",
        "\n",
        "# 임의의 문장을 사용해 생성된 새로운 문장 확인\n",
        "test_sentence = \"이날은 수수개비를 꺾어도 아이들은 매를 맞지 않는다\"\n",
        "\n",
        "next_words = 100\n",
        "\n",
        "for _ in range(next_words):\n",
        "  test_text_x = test_sentence.split(' ')[-seq_length:]\n",
        "  test_text_x = np.array([word2idx[c] if c in word2idx else word2idx['UNK'] for c in test_text_x])\n",
        "  test_text_x = pad_sequences([test_text_x], maxlen=seq_length, padding='pre', value=word2idx['UNK'])\n",
        "  output_idx = np.argmax(model.predict(test_text_x)[0])  # 출력값 중에서 가장 값이 큰 인덱스 반환\n",
        "  test_sentence += ' ' + idx2word[output_idx]  # 출력 단어는 test_sentence에 누적해 다음 작업의 입력으로 활용\n",
        "\n",
        "  print()\n",
        "print(test_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVnWuL2Q3gsH",
        "outputId": "d8522433-e462-48b0-d488-a8def03a63a5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 652ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "\n",
            "이날은 수수개비를 꺾어도 아이들은 매를 맞지 않는다 논이 지는 아니다 일이 밤에는 죽고 손이 손목 이고 향해 고개가 조그마한 생각하니 시절보고 자식 빌붙어서 아침 그는 넘쳐 찔렀으면 알 잘 눌리어 이고 말 연못에 말도 언제 죄를 있다가 삼 맴을 말하자면 이 강포수 내 두만네 계신데 싸울 욕설을 화적떼한테 서 내가 소리소리지르고 될 자신도 떡을 것처럼 \n",
            " 틈바구니에서 들판의 우회한다 위에서 걷어서 날 했으나 와본 보고 두 다가 내 못헐 나간 월선이 \n",
            " 부터 1장 와서 없는 애 죽음의 것도 고 증을 없었다 \n",
            " 겉소 간장을 있는 것은 아닐 강포 찾아서 \n",
            " 요새 좀 두 리노 윤보는 시켰을 갔다든가? 자식 감는다 밖을 나갔다온다믄서 가까이 정신이 했으며, 어른이니께 최참판댁\n"
          ]
        }
      ]
    }
  ]
}